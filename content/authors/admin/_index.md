---
# Display name
title: Yihe Deng

# Name pronunciation (optional)
name_pronunciation: ''

# Full name (for SEO)
first_name: Shiung Wu
last_name: Chien

# Status emoji
status:
  icon: üìù

# Is this the primary user of the site?
superuser: true

# Highlight the author in author lists? (true/false)
highlight_name: true

# Role/position/tagline
role: PhD Student in LLM

# Organizations/Affiliations to display in Biography blox
organizations:
  - name: UCLA
    url: https://www.ucla.edu/

# Social network links
# Need to use another icon? Simply download the SVG icon to your `assets/media/icons/` folder.
profiles:
  # - icon: at-symbol
  #   url: 'mailto:your-email@example.com'
  #   label: E-mail Me
  - icon: brands/x
    url: https://x.com/Yihe__Deng
  - icon: brands/github
    url: https://github.com/yihedeng9
  - icon: brands/linkedin
    url: https://www.linkedin.com/in/yihe-deng-1895bb250/
  - icon: academicons/google-scholar
    url: https://scholar.google.com/citations?user=7Lix1poAAAAJ&hl=en

interests:
  - Large Language Models (LLMs)
  - LLM Post-training / Self-training
  - Multi-modal Learning

education:
  - area: PhD Computer Science
    institution: University of California, Los Angeles
    date_start: 2021-09
    date_end: 2026-06
    summary: |
      I'm currently a 4th-year Ph.D. student at Department of Computer Science, University of California, Los Angeles (UCLA), where I am very fortunate to be advised by [Prof. Wei Wang](https://web.cs.ucla.edu/~weiwang/).
    # button:
    #   text: 'Read Thesis'
    #   url: 'https://example.com'
  - area: MS Computer Science
    institution: University of California, Los Angeles
    date_start: 2019-09
    date_end: 2021-06
    summary: |
      I've been an student researcher at [UCLA-NLP](https://web.cs.ucla.edu/~kwchang/members/) group with [Prof. Kai-Wei Chang](https://web.cs.ucla.edu/~kwchang/).

      Courses included:
      - Natural Language Processing
      - Advanced Data Mining
      - Machine Learning Algorithms;
  - area: BS Mathematics of Computation
    institution: University of California, Los Angeles
    date_start: 2015-09
    date_end: 2019-06
    summary: |
      Courses included:
      - Probability Theory
      - Optimization
      - Monte Carlo Methods
work:
  - position: Applied Scientist Intern
    company_name: Amazon Web Service (AWS)
    company_url: ''
    company_logo: ''
    date_start: 2023-06
    date_end: 2023-09
    summary: |2-
      Large Language Model Reasoning with Knowledge Graphs
      - Enhance the reasoning of LLMs with the multi-modal input of knowledge graphs.
  # - position: Backend Software Engineer
  #   company_name: X
  #   company_url: ''
  #   company_logo: ''
  #   date_start: 2016-01-01
  #   date_end: 2020-12-31
  #   summary: |
  #     Responsibilities include:
  #     - Migrated infrastructure to a new data center
  #     - lorem ipsum dolor sit amet, consectetur adipiscing elit
  #     - lorem ipsum dolor sit amet, consectetur adipiscing elit

# Skills
# Add your own SVG icons to `assets/media/icons/`
skills:
  - name: Technical Skills
    items:
      - name: Python
        description: ''
        percent: 80
        icon: code-bracket
      - name: Data Science
        description: ''
        percent: 100
        icon: chart-bar
      # - name: SQL
      #   description: ''
      #   percent: 40
      #   icon: circle-stack
  - name: Hobbies
    color: '#eeac02'
    color_border: '#f0bf23'
    items:
      - name: Hiking
        description: ''
        percent: 60
        icon: person-simple-walk
      - name: Cats
        description: ''
        percent: 100
        icon: cat
      - name: Photography
        description: ''
        percent: 80
        icon: camera

languages:
  - name: English
    percent: 95
  - name: Chinese
    percent: 100

# Awards.
#   Add/remove as many awards below as you like.
#   Only `title`, `awarder`, and `date` are required.
#   Begin multi-line `summary` with YAML's `|` or `|2-` multi-line prefix and indent 2 spaces below.
# awards:
#   - title: NeurIPS 2023 Scholar Award
#     url: ''
#     date: '2023'
#     awarder: Neu
#     icon: coursera
#     summary: |
#       I studied the foundational concept of neural networks and deep learning. By the end, I was familiar with the significant technological trends driving the rise of deep learning; build, train, and apply fully connected deep neural networks; implement efficient (vectorized) neural networks; identify key parameters in a neural network‚Äôs architecture; and apply deep learning to your own applications.

---

## About Me

My research interests focus on post-training for Large Language Models (LLMs). Specifically, I'm interested in aspects including alignment fine-tuning, self-training and hallucination problems. I also work on robustness and multi-modal learning.  